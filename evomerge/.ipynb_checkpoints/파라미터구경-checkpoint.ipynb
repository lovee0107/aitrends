{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c469e16e-ed6a-48c0-a71b-c1008e3ee822",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe70e7c9-6f1d-48ef-9931-1b510163f1f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'evomerge'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 23\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# python merge_front.py --config_path configs/llm/merge/shisa-wizard.yaml\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 아래는 shisa-wizard.yaml 여기 파일에 있는 정보\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# merge.py\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01margparse\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mevomerge\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_config\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mevomerge\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmerge\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m merge_models\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmain\u001b[39m(args):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'evomerge'"
     ]
    }
   ],
   "source": [
    "python merge_front.py --config_path configs/llm/merge/shisa-wizard.yaml\n",
    "\n",
    "아래는 shisa-wizard.yaml 여기 파일에 있는 정보\n",
    "population:\n",
    "  - model_path: /Users/kooksunho/.cache/huggingface/hub/models--augmxnt--shisa-gamma-7b-v1/snapshots/9fae830099495b79bda534c73e628ed7169fe363\n",
    "  - model_path: /Users/kooksunho/.cache/huggingface/hub/models--WizardLM--WizardMath-7B-V1.1/snapshots/1b955017f4f5b5012fb14109740576e34e5268fc\n",
    "\n",
    "output_path: merged-models/shisa-wizard-merged-slerp\n",
    "\n",
    "merge_config:\n",
    "  merge_method: linear   # or 'linear' for 간단 평균\n",
    "  num_generation: 1\n",
    "  num_population: 2\n",
    "  num_trials: 4\n",
    "  eval_config:\n",
    "    config_path: configs/llm/shisa-wizard-eval.yaml\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# merge.py\n",
    "import argparse\n",
    "from evomerge import load_config\n",
    "from evomerge.merge import merge_models\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    config = load_config(args.config_path)\n",
    "    merge_models(config)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--config_path\", type=str, required=True, help=\"Path to the merge config file\")\n",
    "    args = parser.parse_args()\n",
    "    main(args)\n",
    "\n",
    "# evomerge/merge.py\n",
    "import os, torch, copy\n",
    "from .models.causallm import CausalLMWithTransformers\n",
    "\n",
    "def merge_models(config):\n",
    "    model_paths = [m[\"model_path\"] for m in config[\"population\"]]\n",
    "    output_path = config[\"output_path\"]\n",
    "    method = config[\"merge_config\"].get(\"merge_method\", \"slerp\")\n",
    "\n",
    "\n",
    "    print(f\"🚀 Merging models: {model_paths} with method: {method}\")\n",
    "\n",
    "    base = CausalLMWithTransformers(\n",
    "        model_path=model_paths[0],\n",
    "        template=\"ja-alpaca-cot\",\n",
    "        model_kwargs={\"torch_dtype\": torch.float16}  # 👈 추가!\n",
    "    )\n",
    "    others = [\n",
    "        CausalLMWithTransformers(\n",
    "            model_path=p,\n",
    "            template=\"ja-alpaca-cot\",\n",
    "            model_kwargs={\"torch_dtype\": torch.float16}  # 👈 추가!\n",
    "        )\n",
    "        for p in model_paths[1:]\n",
    "    ]\n",
    "    \n",
    "    base_state = base.model.state_dict()\n",
    "    other_states = [m.model.state_dict() for m in others]\n",
    "\n",
    "\n",
    "    merged_state = copy.deepcopy(base_state)\n",
    "    print(merged_state.shape())\n",
    "    print(\"======================\")\n",
    "    for k in merged_state:\n",
    "        #print(k)\n",
    "        print(\"======================\")\n",
    "        if all(k in o for o in other_states):\n",
    "            for o in other_states:\n",
    "                #print(o)\n",
    "                print(\"======================\")\n",
    "                if method == \"linear\":\n",
    "                    #merged_state[k] += o[k]\n",
    "                    pass\n",
    "                elif method == \"slerp\":\n",
    "                    #merged_state[k] = 0.5 * merged_state[k] + 0.5 * o[k]\n",
    "                    pass\n",
    "            if method == \"linear\":\n",
    "                #merged_state[k] /= (len(other_states) + 1)\n",
    "                pass\n",
    "            \n",
    "    print(merged_state[\"model.embed_tokens.weight\"])\n",
    "    exit()\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    torch.save(merged_state, os.path.join(output_path, \"pytorch_model.bin\"))\n",
    "    print(f\"[✓] Merged model saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec91ae28-bd4b-43f1-8638-2fff1f17b5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "import copy\n",
    "\n",
    "# 모델 경로 설정\n",
    "shisa_path = \"/Users/kooksunho/.cache/huggingface/hub/models--augmxnt--shisa-gamma-7b-v1/snapshots/9fae830099495b79bda534c73e628ed7169fe363\"\n",
    "wizard_path = \"/Users/kooksunho/.cache/huggingface/hub/models--WizardLM--WizardMath-7B-V1.1/snapshots/1b955017f4f5b5012fb14109740576e34e5268fc\"\n",
    "\n",
    "# 모델 로드\n",
    "shisa_model = AutoModelForCausalLM.from_pretrained(shisa_path, torch_dtype=torch.float16)\n",
    "wizard_model = AutoModelForCausalLM.from_pretrained(wizard_path, torch_dtype=torch.float16)\n",
    "\n",
    "# 상태 딕셔너리 추출\n",
    "shisa_state = shisa_model.state_dict()\n",
    "wizard_state = wizard_model.state_dict()\n",
    "\n",
    "# 병합 방식: linear or slerp\n",
    "merge_method = \"slerp\"  # 또는 \"linear\"\n",
    "\n",
    "# 병합 수행\n",
    "merged_state = copy.deepcopy(shisa_state)\n",
    "\n",
    "for k in merged_state:\n",
    "    if k in wizard_state:\n",
    "        if merge_method == \"linear\":\n",
    "            #merged_state[k] = (shisa_state[k] + wizard_state[k]) / 2\n",
    "        elif merge_method == \"slerp\":\n",
    "            #merged_state[k] = 0.5 * shisa_state[k] + 0.5 * wizard_state[k]\n",
    "\n",
    "# 예시로 임베딩 레이어의 파라미터 비교\n",
    "print(\"Shisa embed_tokens.weight:\")\n",
    "print(shisa_state[\"model.embed_tokens.weight\"][0][:10])\n",
    "\n",
    "print(\"\\nWizard embed_tokens.weight:\")\n",
    "print(wizard_state[\"model.embed_tokens.weight\"][0][:10])\n",
    "\n",
    "print(\"\\nMerged embed_tokens.weight:\")\n",
    "print(merged_state[\"model.embed_tokens.weight\"][0][:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cropnet_env)",
   "language": "python",
   "name": "cropnet_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
